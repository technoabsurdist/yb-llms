# On Limitations of the Transformer Architecture - YouTube -- Transcript

## Summary
The text explores the limitations of Transformers in artificial intelligence, particularly in the context of function composition tasks and compositional challenges. The author discusses how Transformers tend to produce hallucinations, generating incorrect answers unrelated to the input. The text delves into theoretical limitations of Transformers, their handling of function composition, and their probabilistic nature as language generators. The section summaries discuss the challenges faced by Transformers in computation, sequence composition, and memory usage, shedding light on their limitations in handling complex tasks. The author also explores potential solutions like the Chain of Thought cut approach and highlights the need for further research to understand and improve Transformer architecture.

## Bullet Points
- Transformer architecture in AI can produce hallucinations, generating answers that do not align with training data or questions.
- Research explores the limitations of Transformers, including difficulty in function composition and sequential composition tasks.
- Transformers struggle with compositional tasks like multiplication and logic puzzles, requiring significant computational resources.
- Complexity arguments highlight limitations of Transformers in handling tasks beyond certain thresholds, such as function composition or circuit evaluation.
- Challenges in computational complexity indicate a fundamental mismatch between transformer architecture and certain problem types.

## Formatted Transcript
Section Introduction

In this section, we delve into the intriguing yet challenging aspect of the Transformer architecture in artificial intelligence, which is its tendency to produce hallucinations. This means that sometimes the answers generated by these models don't align with the information they were trained on or the question asked. There's a lot of research exploring the nature types and solutions to these hallucinations. One might wonder if the root of this issue lies within the Transformer architecture itself. Previous studies, including a notable one from 2020, have highlighted theoretical limitations of Transformers such as their inability to recognize simple patterns like whether a sentence has an even number of negations or if parentheses are balanced. These limitations, while proven through complex theoretical models, seem to only emerge with very large inputs which are not common in practical scenarios. In fact, it has been shown that Transformers can handle these tasks well within realistic settings.

Our exploration also touches on how Transformers are viewed through the lens of complexity theory, revealing that they fall into a relatively weak complexity class. This perspective helps us understand the computational boundaries of Transformers. Additionally, recent research has pinpointed specific mathematical challenges that Transformers struggle with, such as identifying three numbers in a sequence that sum up to zero. This finding contrasts with their ability to solve simpler versions of the problem and highlights a unique advantage of Transformers over other machine learning models. Even though these challenges are not directly related to their primary functions, we are particularly interested in a fundamental limitation of the Transformer architecture: its difficulty with function composition, a basic yet crucial semantic operation. For instance, when asked about the birthday of Frederick Chan's father based on given facts, a Transformer might fail to connect the dots correctly. This is a clear example of function composition where the model needs to combine relational information effectively. Some have suggested integrating knowledge graphs with Transformers to improve their ability to perform function composition and reduce hallucinations. Function composition is not only vital for combining relational data but also plays a key role in language understanding, which is a core strength of Transformers. The handling of indexicals or words that refer to entities within a specific context further illustrates the importance of function composition and understanding language. However, Transformers sometimes struggle with this, indicating a potential area for improvement.

In our paper, we demonstrate that Transformers inherently struggle with function composition. Specifically, we show that a single Transformer attention layer is unlikely to compute function composition queries correctly if the size of the function's domain and certain other parameters exceed a particular threshold. This limitation seems to stem from the softmax computation within the Transformer, which relies on limited non-local information to compute the next token's embedding. While this issue is significant, we also discuss how the Chain of Thought cut approach can mitigate the problem by breaking down tasks into smaller steps. However, we also present a theorem suggesting that even with COT, a Transformer layer would require a significantly larger prompt to handle a series of function compositions. Furthermore...

Section Summary

In this section, we explore the limitations of the Transformer architecture, particularly in its ability to perform function composition tasks. Reliably, we demonstrate that Transformers struggle with composing functions accurately, especially when the domain size exceeds a certain threshold due to inherent weaknesses in their attention mechanisms. Additionally, we highlight that Transformers face challenges in tasks requiring sequential composition of elementary tasks such as multiplying multi-digit integers, which exacerbates with the depth of composition required.

Section Preliminary Definitions

In this section, we begin by laying out some foundational concepts necessary to understand our discussion on Transformers, function composition, and information theory. Firstly, we delve into the concept of a transformer to mathematically describe how Transformers operate. We slightly modify an existing formal model. At the heart of a transformer is the self-attention unit, which is essentially a function that transforms a sequence of input vectors into a sequence of output vectors, maintaining the sequence length but potentially altering the dimensionality of the vectors. This transformation is achieved through the use of three matrices known as the key, query, and value matrices, which are all of the same dimension. For simplicity in our explanation, when a sequence of vectors is fed into the self-attention unit, it computes a new sequence where each element is a weighted sum of the original vectors with the weights determined by the attention mechanism. We also introduce the concept of an H-headed Transformer layer, which consists of multiple self-attention units working in parallel on the same input but potentially capturing different aspects of the input data. The outputs of these units are then combined to produce a single output vector for each input vector in the sequence. A Transformer model is built by stacking several of these layers on top of each other.

Moving on, we define the problem of function composition. Imagine we have two functions where one maps elements from one domain to another and the second function maps elements of the second domain to a third domain. We describe these functions in a structured prompt divided into three parts. The first part describes the first function, the second part describes the second function, and the third part poses a query about the composition of these two functions. We say that a Transformer layer correctly computes the function composition if it can provide the correct answer to the query based on the descriptions of the two functions.

In the realm of information theory, we stick to standard terminology. We discuss concepts like entropy, which measures the uncertainty of a random variable, and mutual information, which quantifies the amount of information shared between two random variables. These concepts play a crucial role in our analysis. Lastly, we tackle the challenge of proving the impossibility of certain computations by Transformers under specific conditions. We demonstrate that if a Transformer layer does not have sufficient capacity measured in terms of the number of heads, the dimensionality of the embeddings, and the precision of computation, it cannot correctly solve the function composition problem for arbitrary functions. This conclusion is drawn from principles in communication complexity, which studies the minimum amount of information exchange required for distributed agents to compute a function.

Section Chain of Thought

In this section, we explore whether the Chain of Thought cut approach can assist in solving complex problems by breaking them down into simpler steps. We believe that the answer is yes. For instance, when faced with a question about the birthplace of Turing, we can simplify the problem by creating a COT prompt that divides the question into easier parts, such as determining where Turing was born, identifying the country of that place, and then concluding Turing's country of birth. However, we demonstrate that solving more complex problems, which involve applying multiple functions in sequence, may require an indefinitely large number of C steps. Specifically, we look at problems where we have to apply a series of functions one after the other to a starting value. Imagine we have a set of functions and we apply the first function to a number, then apply the second function to the result of the first function, and so on until we have applied all functions.

We then delve into the technical requirements for a Transformer layer, a type of neural network architecture, to solve these iterated function composition problems using COT. We consider factors such as the number of attention heads in the Transformer, the dimension of the embeddings, the precision of the computations, and the size of the problem domain. We find that the number of C steps needed for a Transformer layer to correctly answer these problems is proportional to the square root of the ratio of the problem's domain size to the product of the number of attention heads, the embedding dimension, and the computation precision.

To support our findings, we draw parallels with a known problem in communication complexity called pointer chasing. In this problem, two parties, Alice and Bob, each know a part of a sequence of functions, and they need to communicate to find out the result of applying these functions in sequence. We show that if a Transformer layer can solve the iterated function composition problem within a certain number of C steps, then Alice and Bob can use a similar strategy to solve the pointer chasing problem by exchanging information about the results of applying their functions. We outline a communication protocol where Alice and Bob take turns simulating the steps of COT, exchanging information about the application of their functions. This process continues...

Section Compositionality and Logarithmic Space

In this section, we delve into the concept of compositionality and its challenges within the realm of logarithmic space. Recently, a study uncovered a new type of error made by Transformer models. They struggle with compositional tasks. These tasks involve performing basic operations repeatedly, such as multiplying multi-digit numbers, solving a sum maximization problem with specific constraints, and tackling logic puzzles like Einstein's riddle. These errors are intriguing because they highlight a potential limitation in how these models process information.

When we talk about evaluating a circuit, we're referring to the process of determining the output based on the inputs and the operations found within the circuit. This could be as simple as multiplying numbers or as complex as solving optimization problems with certain rules. Another concept we explore is derivability, which involves finding a sequence of steps from an initial state to a final state within a given set of rules. This is akin to solving a puzzle where each move must be legal to reach the end goal.

Logical reasoning, particularly with logic puzzles, is typically framed as a satisfiability problem, which is known to be quite challenging. Despite this, there are simpler cases like 2-SAT, Horn-SAT, and mod 2-SAT, which are more manageable and form the basis of much common sense reasoning. However, we question whether Transformer models can handle even these simpler cases effectively.

We observe that the computation performed by a multi-layer Transformer on a given input can be done using a logarithmic amount of memory space. This isn't a new discovery, but it's important because it suggests that Transformers operate within a certain computational complexity class. This limitation in memory usage is crucial when we consider the ability of these models to handle tasks that require more complex reasoning or deeper levels of compositionality.

We further discuss that if we accept certain widely held beliefs in computational complexity, it becomes clear that multi-layer Transformers cannot solve certain problems unless some of these beliefs are proven wrong. Specifically, problems like derivability, 2-SAT, Horn-SAT, and circuit evaluation are out of reach unless we find that deterministic and non-deterministic logarithmic memory are equivalent or even that deterministic logarithmic memory is as powerful as polynomial time. These complexity results...

Section Discussion

In this section, we delve into how we've applied two types of complexity arguments, communication complexity and computational complexity, to highlight certain limitations of the Transformer architecture, which lead to various kinds of errors in output, often referred to as hallucinations. We've demonstrated that a single layer of a Transformer can't solve the basic function composition problem. Moreover, we found that Chain of Thought can tackle the iterated composition problem only if it generates a prompt that's significantly long. Specifically, the length must be proportional to the square root of n. These mathematical insights have their limitations. Firstly, they apply when the size of the function's domain is larger than the Transformer's dimension parameters, which are usually in the hundreds. And secondly, these insights don't hold when considering multiple layers of Transformers.

We also explored evidence from the realm of complexity theory, specifically looking at classical Turing machines, to show that the tasks which are empirically challenging for Transformers involve computational elements that are inherently difficult for Transformers to process. It's important to note that the complexity arguments we use come with certain caveats. The claim that composition is impossible for a single layer holds in a probabilistic sense. The error probability is not zero but also not certain and only when the function's domain exceeds the Transformer layer's parameters. The arguments based on computational complexity come with their own set of conditions. They depend on certain conjectures that, while widely accepted, remain unproven, and these results are asmically, meaning they apply to instances beyond a certain unknown size where these conjectures are assumed to be relevant.

These complexity results serve as cautionary tales, suggesting a fundamental mismatch between these problems and the Transformer architecture, indicating that we shouldn't expect these issues to be solvable indefinitely in practice. However, similar to other complexity conjectures like P not equal to NP, computational challenges often arise even with reasonably small instances. For instance, we've observed that Transformers struggle with relatively small compositionality tasks, and we've provided anecdotal evidence in the appendix showing that large language models frequently make errors in response to small prompts related to function composition. Interestingly, the opposite is also true for some nitrogen monophosphide complete problems like 3-SAT, which seem solvable in practice for large instances thanks to decades of exploring a wide range of algorithmic techniques tailored to practical instances rather than a fixed algorithmic architecture.

The real issue with complexity lower bounds isn't their provisional and asmically nature. It's that they are rare, difficult to establish, come in limited forms, and tend to overestimate the capabilities of computational agents. For example, Lemo 1 is designed to hold even under sophisticated mathematical encoding by an agent named Grace. However, when applied in Theorem 1, Grace's message to Xavier is not a complex coding of her tokens but rather two simple numerical expressions. This simplistic approach to encoding the values of a function highlights an open problem and the potential to develop a more sophisticated version of communication complexity for computationally limited agents aiming to better understand the limitations of architectures like the Transformer.

Our exploration of these two genres of negative results poses an interesting challenge: What would it take to design an attention layer immune to these lower-bound techniques while keeping the architecture's efficiency and effectiveness? Our findings suggest a version of the softmax computation in the attention layer that is either not commutative or not associative or one that requires more than logarithmic space. However, evading a lower or bound technique doesn't automatically translate to improved performance.

Acknowledgement

We are thankful to Fernando Pereira for his valuable insights, engagement, and constructive criticism throughout this project. We also appreciate Olivier Bousquet for his insightful comments on an earlier draft. This work was conducted while the third author was a visiting scientist at Google Deep Mind in Zurich, and the efforts of the first and third authors were partially supported by an NS Grant.
